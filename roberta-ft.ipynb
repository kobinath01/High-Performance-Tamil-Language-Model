{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n# Install necessary libraries\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl\n%pip install -U torchao\n%pip install -U evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:55:35.787970Z","iopub.execute_input":"2025-01-07T14:55:35.788265Z","iopub.status.idle":"2025-01-07T14:56:17.755141Z","shell.execute_reply.started":"2025-01-07T14:55:35.788244Z","shell.execute_reply":"2025-01-07T14:56:17.754045Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport wandb\nimport numpy as np\nimport collections\nimport evaluate\nfrom tqdm.auto import tqdm\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig, \n    TrainingArguments, \n    logging\n)\nfrom peft import LoraConfig, get_peft_model\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom trl import SFTTrainer, setup_chat_format\nimport bitsandbytes as bnb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:56:17.756625Z","iopub.execute_input":"2025-01-07T14:56:17.756953Z","iopub.status.idle":"2025-01-07T14:56:29.385575Z","shell.execute_reply.started":"2025-01-07T14:56:17.756924Z","shell.execute_reply":"2025-01-07T14:56:29.384873Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Kaggle secrets setup\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"roberta\")\nlogin(token=hf_token)\nwb_token = user_secrets.get_secret(\"robertaw\")\n\n# Wandb initialization for tracking\nwandb.login(key=wb_token)\nrun = wandb.init(project='Fine-tune_Roberta', job_type=\"training\", anonymous=\"allow\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:56:29.387230Z","iopub.execute_input":"2025-01-07T14:56:29.387894Z","iopub.status.idle":"2025-01-07T14:56:41.961331Z","shell.execute_reply.started":"2025-01-07T14:56:29.387865Z","shell.execute_reply":"2025-01-07T14:56:41.960506Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkobinatha-20\u001b[0m (\u001b[33mkobinatha-20-student\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250107_145635-effm946y</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kobinatha-20-student/Fine-tune_Roberta/runs/effm946y' target=\"_blank\">sleek-thunder-5</a></strong> to <a href='https://wandb.ai/kobinatha-20-student/Fine-tune_Roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kobinatha-20-student/Fine-tune_Roberta' target=\"_blank\">https://wandb.ai/kobinatha-20-student/Fine-tune_Roberta</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kobinatha-20-student/Fine-tune_Roberta/runs/effm946y' target=\"_blank\">https://wandb.ai/kobinatha-20-student/Fine-tune_Roberta/runs/effm946y</a>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Load Dataset and Tokenizer\nmodel_checkpoint = \"deepset/xlm-roberta-large-squad2\"\ndataset_name = \"RajeevanL/tamil_squad-2.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:56:41.962364Z","iopub.execute_input":"2025-01-07T14:56:41.962684Z","iopub.status.idle":"2025-01-07T14:56:45.715173Z","shell.execute_reply.started":"2025-01-07T14:56:41.962653Z","shell.execute_reply":"2025-01-07T14:56:45.714416Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd3d7f6380e4dac9eaadf68b8aa4080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/606 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c70cf81b8e4f1ebb5cc4cb23d13018"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f87e1492810643fe98f1bc0c4ca97ec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"705cd4c5644d4bd9b5578f99e3c60fa9"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Load the dataset from Hugging Face Hub\ndataset = load_dataset(dataset_name)\ntrain_dataset = dataset[\"train\"]\nvalidation_dataset = dataset[\"validation\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:57:11.518100Z","iopub.execute_input":"2025-01-07T14:57:11.518456Z","iopub.status.idle":"2025-01-07T14:57:15.809505Z","shell.execute_reply.started":"2025-01-07T14:57:11.518429Z","shell.execute_reply":"2025-01-07T14:57:15.808652Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/573 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc243318ede44c66b00cbcfdbbe3e1df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/59.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe68bc6530784d339f75a9f82b0a75bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/5.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9375b29fa564441a7bbcf9cdfda7e0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/5.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ee001c48c824a88bf6c724544b5206f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/66277 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5c5a7716e784b4eb3bed420409622c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5848 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffb15d363346436ab7cc3adf7cf2346f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5848 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"714ab4cab0354219963925301b4cea92"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(train_dataset[0]['Answer'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T15:27:49.693594Z","iopub.execute_input":"2025-01-07T15:27:49.693953Z","iopub.status.idle":"2025-01-07T15:27:49.724587Z","shell.execute_reply.started":"2025-01-07T15:27:49.693926Z","shell.execute_reply":"2025-01-07T15:27:49.723370Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-88f0cfaf3df5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyError\u001b[0m: 'Answer'"],"ename":"KeyError","evalue":"'Answer'","output_type":"error"}],"execution_count":49},{"cell_type":"code","source":"# Preprocessing Constants\nmax_length = 384\nstride = 128\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T15:23:57.263152Z","iopub.execute_input":"2025-01-07T15:23:57.263447Z","iopub.status.idle":"2025-01-07T15:23:57.268204Z","shell.execute_reply.started":"2025-01-07T15:23:57.263426Z","shell.execute_reply":"2025-01-07T15:23:57.267315Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def preprocess_training_examples(examples):\n    # Ensure questions are properly formatted as strings\n    questions = [str(q).strip() for q in examples[\"Question\"]]\n    \n    # Ensure context is properly formatted as strings\n    contexts = [str(c).strip() for c in examples[\"Context\"]]\n    \n    inputs = tokenizer(\n        questions,\n        contexts,\n        max_length=384,\n        truncation=True,\n        padding=\"max_length\",\n        return_offsets_mapping=True,\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    answers = examples[\"Answer\"]\n\n    start_positions = []\n    end_positions = []\n\n    for i, offsets in enumerate(offset_mapping):\n        answer = answers[i]\n\n        # Handle the case where answer is None\n        if answer is None:\n            start_char = end_char = -1  # Set to -1 for invalid answer\n        elif isinstance(answer, dict):  # Check if answer is a dictionary\n            start_char = answer[\"answer_start\"]\n            end_char = start_char + len(answer[\"text\"])\n        else:  # If answer is a string, adjust accordingly\n            start_char = answer.find(answer) if answer else -1  # Set to -1 if answer is empty\n            end_char = start_char + len(answer) if start_char != -1 else -1\n\n        # Find start and end token positions\n        sequence_ids = inputs.sequence_ids(i)\n        context_start = sequence_ids.index(1)\n        context_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n\n        # Adjust the start and end positions\n        start_token = context_start\n        end_token = context_end\n        for idx, (start, end) in enumerate(offsets):\n            if start <= start_char < end:\n                start_token = idx\n            if start < end_char <= end:\n                end_token = idx\n                break\n\n        start_positions.append(start_token)\n        end_positions.append(end_token)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T15:23:58.023295Z","iopub.execute_input":"2025-01-07T15:23:58.023605Z","iopub.status.idle":"2025-01-07T15:23:58.032056Z","shell.execute_reply.started":"2025-01-07T15:23:58.023580Z","shell.execute_reply":"2025-01-07T15:23:58.031217Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def preprocess_validation_examples(examples):\n    questions = [q.strip() for q in examples[\"Question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"Context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T15:23:58.918249Z","iopub.execute_input":"2025-01-07T15:23:58.918562Z","iopub.status.idle":"2025-01-07T15:23:58.924675Z","shell.execute_reply.started":"2025-01-07T15:23:58.918537Z","shell.execute_reply":"2025-01-07T15:23:58.923893Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Apply Preprocessing\ntrain_dataset = train_dataset.map(preprocess_training_examples, batched=True, remove_columns=train_dataset.column_names)\nvalidation_dataset = validation_dataset.map(preprocess_validation_examples, batched=True, remove_columns=validation_dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T15:24:04.638722Z","iopub.execute_input":"2025-01-07T15:24:04.639048Z","iopub.status.idle":"2025-01-07T15:24:04.727671Z","shell.execute_reply.started":"2025-01-07T15:24:04.639025Z","shell.execute_reply":"2025-01-07T15:24:04.726572Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/66277 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304b48ac42774349938bd19315288813"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-7b99a5580db0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Apply Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_training_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_validation_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m         }\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3071\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3072\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3073\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3074\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3474\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   3475\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3476\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   3477\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3336\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m                 processed_inputs = {\n","\u001b[0;32m<ipython-input-46-067d1313c359>\u001b[0m in \u001b[0;36mpreprocess_training_examples\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_training_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Ensure questions are properly formatted as strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Ensure context is properly formatted as strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys_to_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Question'"],"ename":"KeyError","evalue":"'Question'","output_type":"error"}],"execution_count":48},{"cell_type":"code","source":"print(train_dataset[0]['Answer'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T15:10:16.355731Z","iopub.execute_input":"2025-01-07T15:10:16.356030Z","iopub.status.idle":"2025-01-07T15:10:16.362165Z","shell.execute_reply.started":"2025-01-07T15:10:16.356008Z","shell.execute_reply":"2025-01-07T15:10:16.361243Z"}},"outputs":[{"name":"stdout","text":"விடுதலை செய்பவர்கள்.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Define Metrics\nmetric = evaluate.load(\"squad\")\n\ndef compute_metrics(start_logits, end_logits, features, examples):\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    for example in tqdm(examples):\n        example_id = example[\"id\"]\n        context = example[\"Context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip answers that are not fully in the context\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # Skip answers with a length that is either < 0 or > max_answer_length\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"Answer\"]} for ex in examples]\n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n\n# Load Pretrained Model\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Arguments\nargs = TrainingArguments(\n    \"xlm-roberta-finetuned-tamil-squad\",\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    fp16=True,\n    push_to_hub=False,\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_training_examples(examples):\n    questions = [q.strip() for q in examples[\"Question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"Context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"Answer\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:46:24.520681Z","iopub.execute_input":"2025-01-07T12:46:24.521013Z","iopub.status.idle":"2025-01-07T12:46:24.535609Z","shell.execute_reply.started":"2025-01-07T12:46:24.520982Z","shell.execute_reply":"2025-01-07T12:46:24.534955Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the dataset\ndataset_name = \"RajeevanL/tamil_squad-2.0\"\ndf_train = load_dataset(dataset_name, split=\"train\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:49:51.315893Z","iopub.execute_input":"2025-01-07T12:49:51.316197Z","iopub.status.idle":"2025-01-07T12:49:51.950226Z","shell.execute_reply.started":"2025-01-07T12:49:51.316175Z","shell.execute_reply":"2025-01-07T12:49:51.949292Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df_test = load_dataset(dataset_name, split=\"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:50:02.710368Z","iopub.execute_input":"2025-01-07T12:50:02.710730Z","iopub.status.idle":"2025-01-07T12:50:03.396401Z","shell.execute_reply.started":"2025-01-07T12:50:02.710687Z","shell.execute_reply":"2025-01-07T12:50:03.395583Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df_train.column_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:53:33.515528Z","iopub.execute_input":"2025-01-07T12:53:33.515902Z","iopub.status.idle":"2025-01-07T12:53:33.522116Z","shell.execute_reply.started":"2025-01-07T12:53:33.515876Z","shell.execute_reply":"2025-01-07T12:53:33.521316Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['Question', 'Context', 'Answer']"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Access the first row and get the 'question' field\ndf_train[0]['Question']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T12:54:37.840315Z","iopub.execute_input":"2025-01-07T12:54:37.840656Z","iopub.status.idle":"2025-01-07T12:54:37.846653Z","shell.execute_reply.started":"2025-01-07T12:54:37.840627Z","shell.execute_reply":"2025-01-07T12:54:37.845932Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'பெரும்பான்மையான எஸ்டோனியர்கள் ஜெர்மானியர்களை எப்படிக் கருதினர்?'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import numpy as np\nfrom datasets import Dataset\n\n# Add an 'id' column to df_train and df_test using map\ndf_train = df_train.add_column('id', np.linspace(0, len(df_train) - 1, len(df_train)).astype(str))\ndf_test = df_test.add_column('id', np.linspace(0, len(df_test) - 1, len(df_test)).astype(str))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T13:09:10.875624Z","iopub.execute_input":"2025-01-07T13:09:10.875945Z","iopub.status.idle":"2025-01-07T13:09:10.928508Z","shell.execute_reply.started":"2025-01-07T13:09:10.875922Z","shell.execute_reply":"2025-01-07T13:09:10.927890Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the dataset\ndataset_name = \"RajeevanL/tamil_squad-2.0\"\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Format each row\ndef format_row(row):\n    return {\n        \"input_text\": f\"Question: {row['Question']} Context: {row['Context']}\",\n        \"target_text\": row[\"Answer\"],\n    }\n\n# Apply formatting to the entire dataset\nformatted_dataset = dataset.map(format_row)\n\n# Preview the formatted dataset\nprint(formatted_dataset[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments\nfrom trl import SFTTrainer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\n# Load the model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=False,\n)\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\n    base_model, \n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Prepare for LoRA\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"key\", \"value\"],\n    bias=\"none\",\n    task_type=\"QUESTION_ANS\",\n)\n\nlora_model = get_peft_model(model, lora_config)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    \"deepset/xlm-roberta-large-squad2\",\n    trust_remote_code=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Format dataset\ndataset = load_dataset(dataset_name, split=\"train\")\ndef format_row(row):\n    # Ensure 'Context' and 'Answer' are not None\n    if row['Context'] is None or row['Answer'] is None:\n        return None  # Skip rows with missing data\n    \n    # Find the position of the answer in the context\n    answer_start = row['Context'].find(row['Answer'])\n    if answer_start == -1:\n        return None  # Skip rows where the answer is not found in the context\n    \n    answer_end = answer_start + len(row['Answer'])\n\n    # Tokenize the context\n    encoding = tokenizer(row['Context'], truncation=True, padding=True, max_length=512)\n\n    # Ensure start and end positions are valid\n    start_token = encoding.char_to_token(answer_start)\n    end_token = encoding.char_to_token(answer_end - 1)\n\n    # If the positions are invalid, skip the row\n    if start_token is None or end_token is None:\n        return None\n\n    return {\n        \"input_text\": f\"Question: {row['Question']} Context: {row['Context']}\",\n        \"target_text\": row[\"Answer\"],\n        \"start_position\": start_token,\n        \"end_position\": end_token,\n    }\n\n# Apply formatting to the dataset\nformatted_dataset = dataset.map(format_row)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(formatted_dataset.column_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_function(examples):\n    encoding = tokenizer(\n        examples[\"Question\"],  # Tokenize 'Question' column\n        padding=True,\n        truncation=True,\n        max_length=512,\n        add_special_tokens=True,\n    )\n\n    # No need to calculate start and end positions if they are not used\n    return encoding\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training arguments\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    optim=\"adamw_torch_4bit\",\n    save_steps=1000,\n    logging_steps=100,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    fp16=False,\n    bf16=True,\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"wandb\"\n)\n\n# Initialize the trainer with tokenized dataset\ntrainer = SFTTrainer(\n    model=lora_model,\n    args=training_arguments,\n    train_dataset=tokenized_dataset,  # Use the tokenized dataset\n    tokenizer=tokenizer,\n)\n\n# Start the training process\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    optim=\"adamw_torch_4bit\",  # Change to a valid 4-bit optimizer\n    save_steps=1000,\n    logging_steps=100,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    fp16=False,\n    bf16=True,\n    max_grad_norm=0.3,\n    warmup_ratio=0.1,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"wandb\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(formatted_dataset[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ndef format_row(row):\n    # Ensure 'Context' and 'Answer' are not None\n    if row['Context'] is None or row['Answer'] is None:\n        return None  # Skip rows with missing data\n    \n    # Find the position of the answer in the context\n    answer_start = row['Context'].find(row['Answer'])\n    if answer_start == -1:\n        return None  # Skip rows where the answer is not found in the context\n    \n    answer_end = answer_start + len(row['Answer'])\n\n    # Tokenize the context\n    encoding = tokenizer(row['Context'], truncation=True, padding=True, max_length=512)\n\n    # Ensure start and end positions are valid\n    start_token = encoding.char_to_token(answer_start)\n    end_token = encoding.char_to_token(answer_end - 1)\n\n    # If the positions are invalid, skip the row\n    if start_token is None or end_token is None:\n        return None\n\n    return {\n        \"input_text\": f\"Question: {row['Question']} Context: {row['Context']}\",\n        \"target_text\": row[\"Answer\"],\n        \"start_position\": start_token,\n        \"end_position\": end_token,\n    }\n\n# Apply formatting to the dataset\nformatted_dataset = dataset.map(format_row)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_function(examples):\n    # Tokenize the input text\n    encoding = tokenizer(\n        examples[\"input_text\"],  # Use the 'input_text' for tokenization\n        padding=True,\n        truncation=True,\n        max_length=512,\n        add_special_tokens=True,\n    )\n\n    # Prepare lists for the start and end positions\n    start_positions = []\n    end_positions = []\n\n    for i, answer in enumerate(examples[\"Answer\"]):\n        start_pos = examples['start_position'][i]\n        end_pos = examples['end_position'][i]\n\n        # Get the token positions for the start and end positions\n        start_token = encoding.char_to_token(start_pos)\n        end_token = encoding.char_to_token(end_pos - 1)\n\n        # Check if valid tokens are found\n        if start_token is None or end_token is None:\n            # If any token is invalid, set a default value (-1 or None)\n            start_positions.append(-1)  # or use None\n            end_positions.append(-1)    # or None\n        else:\n            # Append valid token positions\n            start_positions.append(start_token)\n            end_positions.append(end_token)\n\n    encoding.update({\n        'start_positions': start_positions,\n        'end_positions': end_positions,\n    })\n\n    return encoding\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}